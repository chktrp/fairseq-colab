{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swPAnAZyHeZq"
      },
      "source": [
        "# Train Q&A using Fairseq on Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DavCwiuHJO6"
      },
      "outputs": [],
      "source": [
        "!pip install fairseq sacrebleu==1.5.1 sacremoses\n",
        "!pip install fastBPE subword_nmt omegaconf hydra-core "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEovL0GdFAsO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2C7dozse332"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg4AARMk7AE4"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJmF2fIvO6X5"
      },
      "outputs": [],
      "source": [
        "%cd /content/fairseq/examples/translation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UQ1p3xr4yji"
      },
      "outputs": [],
      "source": [
        "import csv, random\n",
        "\n",
        "# read the dataset\n",
        "input_files = ['/content/qna_chitchat_caring.tsv',\n",
        "              '/content/qna_chitchat_witty.tsv',\n",
        "              '/content/qna_chitchat_friendly.tsv',\n",
        "              '/content/qna_chitchat_professional.tsv'\n",
        "              ]\n",
        "\n",
        "pair_list = {\n",
        "    'src':[],\n",
        "    'des':[]\n",
        "}\n",
        "\n",
        "n = 0\n",
        "for f in input_files:\n",
        "  with open(f) as file:\n",
        "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
        "    for line in tsv_file:\n",
        "      if len(line) < 2:\n",
        "        # skip if the line doesn't contain the pair of text\n",
        "        continue\n",
        "      pair_list['src'].append(line[0])\n",
        "      pair_list['des'].append(line[1])\n",
        "      n += 1\n",
        "\n",
        "indexes = [i for i in range(n)]\n",
        "random.seed(1)\n",
        "random.shuffle(indexes)\n",
        "\n",
        "tmp_list = pair_list['src'].copy()\n",
        "pair_list['src'] = [tmp_list[i] for i in indexes]\n",
        "\n",
        "tmp_list = pair_list['des'].copy()\n",
        "pair_list['des'] = [tmp_list[i] for i in indexes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AClh6t-5xyS"
      },
      "outputs": [],
      "source": [
        "pair_list['src'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30odJ9ir5-z4"
      },
      "outputs": [],
      "source": [
        "pair_list['des'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq_9qFie6AmQ"
      },
      "outputs": [],
      "source": [
        "# split the read dataset\n",
        "split_ratio = [0, 0.90,0.98,1.00]\n",
        "\n",
        "list_len = len(pair_list['src'])\n",
        "print('number of sample: %d'%(list_len))\n",
        "split_index = [int(x*list_len) for x in split_ratio]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuU1dp7RBjix"
      },
      "outputs": [],
      "source": [
        "dataset_name = 'qna_chitchat'\n",
        "langs = {\n",
        "    'src':'q',\n",
        "    'des':'a'\n",
        "}\n",
        "\n",
        "output_dir = '%s.%s-%s'%(dataset_name, langs['src'], langs['des'])\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(\"%s/tmp\"%output_dir, exist_ok=True)\n",
        "\n",
        "corpus = {}\n",
        "i = 0\n",
        "\n",
        "for s in ['train', 'valid', 'test']:\n",
        "  corpus[s] = {}\n",
        "  for p in pair_list.keys():\n",
        "    corpus[s][p] = pair_list[p][split_index[i]:split_index[i+1]]\n",
        "\n",
        "    with open('%s/tmp/%s.%s'%(output_dir, s, langs[p]), 'w') as f:\n",
        "      f.write('\\n'.join(corpus[s][p]))\n",
        "  i+=1\n",
        "  print('number of %s: %d'%(s, len(corpus[s][p])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMfbJOw88t60"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbscKI1zeLHk"
      },
      "outputs": [],
      "source": [
        "%cd /content/fairseq/examples/translation/\n",
        "!echo 'Cloning Moses github repository (for tokenization scripts)...'\n",
        "!git clone https://github.com/moses-smt/mosesdecoder.git\n",
        "\n",
        "!echo 'Cloning Subword NMT repository (for BPE pre-processing)...'\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sRycz-iecZU"
      },
      "outputs": [],
      "source": [
        "%cd /content/fairseq/examples/translation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tOO6JrzFCAA"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "cd /content/fairseq/examples/translation/\n",
        "\n",
        "SCRIPTS=mosesdecoder/scripts\n",
        "TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl\n",
        "LC=$SCRIPTS/tokenizer/lowercase.perl\n",
        "CLEAN=$SCRIPTS/training/clean-corpus-n.perl\n",
        "BPEROOT=subword-nmt/subword_nmt\n",
        "BPE_TOKENS=10000\n",
        "\n",
        "data_name=qna_chitchat\n",
        "src=q\n",
        "tgt=a\n",
        "lang=$src-$tgt\n",
        "prep=$data_name.$lang\n",
        "tmp=$prep/tmp\n",
        "orig=orig\n",
        "\n",
        "mkdir -p $orig $tmp $prep\n",
        "\n",
        "\n",
        "echo \"pre-processing train data...\"\n",
        "for l in $src $tgt; do\n",
        "    f=$tmp/train.$l\n",
        "    tok=train.tags.$lang.tok.$l\n",
        "\n",
        "    cat $f | \\\n",
        "    perl $TOKENIZER -threads 8 -l $l > $tmp/$tok\n",
        "    echo \"\"\n",
        "done\n",
        "\n",
        "perl $CLEAN -ratio 1.5 $tmp/train.tags.$lang.tok $src $tgt $tmp/train.tags.$lang.clean 1 175\n",
        "for l in $src $tgt; do\n",
        "    perl $LC < $tmp/train.tags.$lang.clean.$l > $tmp/train.tags.$lang.$l\n",
        "done\n",
        "\n",
        "\n",
        "echo \"pre-processing valid data...\"\n",
        "for l in $src $tgt; do\n",
        "    f=$tmp/valid.$l\n",
        "    tok=valid.tags.$lang.tok.$l\n",
        "\n",
        "    cat $f | \\\n",
        "    perl $TOKENIZER -threads 8 -l $l > $tmp/$tok\n",
        "    echo \"\"\n",
        "done\n",
        "\n",
        "perl $CLEAN -ratio 1.5 $tmp/valid.tags.$lang.tok $src $tgt $tmp/valid.tags.$lang.clean 1 175\n",
        "for l in $src $tgt; do\n",
        "    perl $LC < $tmp/valid.tags.$lang.clean.$l > $tmp/valid.tags.$lang.$l\n",
        "done\n",
        "\n",
        "echo \"pre-processing test data...\"\n",
        "for l in $src $tgt; do\n",
        "    f=$tmp/test.$l\n",
        "    tok=test.tags.$lang.tok.$l\n",
        "\n",
        "    cat $f | \\\n",
        "    perl $TOKENIZER -threads 8 -l $l > $tmp/$tok\n",
        "    echo \"\"\n",
        "done\n",
        "\n",
        "perl $CLEAN -ratio 1.5 $tmp/test.tags.$lang.tok $src $tgt $tmp/test.tags.$lang.clean 1 175\n",
        "for l in $src $tgt; do\n",
        "    perl $LC < $tmp/test.tags.$lang.clean.$l > $tmp/test.tags.$lang.$l\n",
        "done\n",
        "\n",
        "TRAIN=$tmp/train.$src-$tgt\n",
        "\n",
        "echo $TRAIN\n",
        "\n",
        "BPE_CODE=$prep/code\n",
        "rm -f $TRAIN\n",
        "for l in $src $tgt; do\n",
        "    cat $tmp/train.$l >> $TRAIN\n",
        "done\n",
        "\n",
        "echo $TRAIN\n",
        "\n",
        "echo \"learn_bpe.py on ${TRAIN}...\"\n",
        "python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $TRAIN > $BPE_CODE\n",
        "\n",
        "for L in $src $tgt; do\n",
        "    for f in train.$L valid.$L test.$L; do\n",
        "        echo \"apply_bpe.py to ${f}...\"\n",
        "        python $BPEROOT/apply_bpe.py -c $BPE_CODE < $tmp/$f > $prep/$f\n",
        "    done\n",
        "done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a7ubL6mmiqL"
      },
      "outputs": [],
      "source": [
        "%cd /content/fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buMZjnViXSU7"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "data_name=qna_chitchat\n",
        "src=q\n",
        "tgt=a\n",
        "lang=$src-$tgt\n",
        "\n",
        "TEXT=examples/translation/$data_name.$src-$tgt\n",
        "\n",
        "rm -rf data-bin/$data_name.tokenized.$src-$tgt\n",
        "\n",
        "fairseq-preprocess --source-lang $src --target-lang $tgt \\\n",
        "    --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\\n",
        "    --destdir data-bin/$data_name.tokenized.$src-$tgt \\\n",
        "    --workers 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjqgupP5ZdUw"
      },
      "outputs": [],
      "source": [
        "%env data_name=qna_chitchat\n",
        "%env src=q\n",
        "%env tgt=a\n",
        "%env lang=$src-$tgt\n",
        "\n",
        "# keep these files for inference later\n",
        "!mkdir -p /content/$data_name.tokenized.$src-$tgt/\n",
        "!cp /content/fairseq/examples/translation/$data_name.$src-$tgt/code \\\n",
        "      /content/$data_name.tokenized.$src-$tgt/\n",
        "!cp -r /content/fairseq/data-bin/$data_name.tokenized.$src-$tgt/dict.$src.txt \\\n",
        "      /content/$data_name.tokenized.$src-$tgt/\n",
        "!cp -r /content/fairseq/data-bin/$data_name.tokenized.$src-$tgt/dict.$tgt.txt \\\n",
        "      /content/$data_name.tokenized.$src-$tgt/\n",
        "%cd /content/\n",
        "!zip -rq $data_name.tokenized.$src-$tgt.zip $data_name.tokenized.$src-$tgt\n",
        "!cp $data_name.tokenized.$src-$tgt.zip \\\n",
        "      /content/runs/fairseq/$src-$tgt/1/$data_name.tokenized.$src-$tgt.zip\n",
        "\n",
        "%cd fairseq/\n",
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train \\\n",
        "    data-bin/$data_name.tokenized.$src-$tgt \\\n",
        "    --save-dir /content/runs/fairseq/$src-$tgt/1/ \\\n",
        "    --arch transformer \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 0.0001 --lr-scheduler inverse_sqrt --warmup-updates 16 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 2048 \\\n",
        "    --no-epoch-checkpoints \\\n",
        "    --skip-invalid-size-inputs-valid-test \\\n",
        "    --max-epoch 256 \\\n",
        "    --encoder-embed-dim 256 \\\n",
        "    --decoder-embed-dim 256 \\\n",
        "    --validate-interval 16 \\\n",
        "    # --finetune-from-model /content/runs/fairseq/$src-$tgt/1/checkpoint_best.pt \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyahGOn_fOD8"
      },
      "outputs": [],
      "source": [
        "%env data_name=qna_chitchat\n",
        "%env src=q\n",
        "%env tgt=a\n",
        "%env lang=$src-$tgt\n",
        "\n",
        "!fairseq-generate \\\n",
        "    data-bin/$data_name.tokenized.$src-$tgt \\\n",
        "    --path /content/runs/fairseq/$src-$tgt/1/checkpoint_best.pt \\\n",
        "    --beam 5 --remove-bpe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHYUR-HVwA2Q"
      },
      "outputs": [],
      "source": [
        "%cd /content/fairseq/\n",
        "\n",
        "%env data_name=qna_chitchat\n",
        "%env src=q\n",
        "%env tgt=a\n",
        "%env lang=$src-$tgt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnp9NBZTp4-l"
      },
      "outputs": [],
      "source": [
        "!pip install --editable ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9yLsBlOqRJK"
      },
      "outputs": [],
      "source": [
        "from fairseq.models.transformer import TransformerModel\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJxbiWovwCYX"
      },
      "outputs": [],
      "source": [
        "train_num = 1\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='fairseq')\n",
        "\n",
        "checkpoint_dir = '/content/runs/fairseq/%s-%s/%s/'%(os.getenv('src'),os.getenv('tgt'),train_num)\n",
        "checkpoint_file = 'checkpoint_best.pt'\n",
        "\n",
        "%cd /content/\n",
        "!cp /content/runs/fairseq/$src-$tgt/1/$data_name.tokenized.$src-$tgt.zip \\\n",
        "      /content/\n",
        "!unzip -f $data_name.tokenized.$src-$tgt.zip\n",
        "\n",
        "qna = TransformerModel.from_pretrained(\n",
        "  checkpoint_dir,\n",
        "  checkpoint_file=checkpoint_file,\n",
        "  data_name_or_path='%s.tokenized.%s-%s'%(os.getenv('data_name'),os.getenv('src'),os.getenv('tgt')),\n",
        "  bpe='subword_nmt',\n",
        "  bpe_codes='%s.tokenized.%s-%s/code'%(os.getenv('data_name'),os.getenv('src'),os.getenv('tgt'))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YieyplPRwf5V"
      },
      "outputs": [],
      "source": [
        "input_text = \"hello, how do you do?\"\n",
        "output_text = qna.translate(input_text)\n",
        "print(output_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "train-fairseq-qna.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
